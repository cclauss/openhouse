"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[671],{7876:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var n=t(5893),i=t(1151);const s={sidebar_position:1},r="Overview",o={id:"intro",title:"Overview",description:"OpenHouse is an open-source control plane designed for efficient management of tables within open data lakehouse",source:"@site/docs/intro.md",sourceDirName:".",slug:"/intro",permalink:"/openhouse/docs/intro",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"docsSidebar",next:{title:"User Guide",permalink:"/openhouse/docs/category/user-guide"}},l={},c=[{value:"Control Plane for Tables",id:"control-plane-for-tables",level:2},{value:"Catalog Service",id:"catalog-service",level:3},{value:"Multi Table Format Support",id:"multi-table-format-support",level:4},{value:"Committing Table Metadata",id:"committing-table-metadata",level:4},{value:"House Table Service",id:"house-table-service",level:3},{value:"Data Services",id:"data-services",level:3},{value:"Jobs Scheduler",id:"jobs-scheduler",level:4},{value:"Jobs Service",id:"jobs-service",level:4},{value:"Engine Integration",id:"engine-integration",level:2},{value:"Deployed System Architecture",id:"deployed-system-architecture",level:2},{value:"Pluggable Architecture",id:"pluggable-architecture",level:2}];function d(e){const a={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(a.h1,{id:"overview",children:"Overview"}),"\n",(0,n.jsxs)(a.p,{children:["OpenHouse is an open-source control plane designed for efficient management of tables within open data lakehouse\ndeployments. The control plane comprises a ",(0,n.jsx)(a.strong,{children:"declarative catalog"})," and a suite of ",(0,n.jsx)(a.strong,{children:"data services"}),". Users can\nseamlessly define Tables, their schemas, and associated metadata declaratively within the catalog.\nOpenHouse reconciles the observed state of Tables with their desired state by orchestrating various\ndata services."]}),"\n",(0,n.jsx)(a.h2,{id:"control-plane-for-tables",children:"Control Plane for Tables"}),"\n",(0,n.jsx)(a.p,{children:"Following figure shows how OpenHouse control plane fits into a broader open source data lakehouse deployment."}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"High Level Overview",src:t(1369).Z+"",width:"510",height:"660"})}),"\n",(0,n.jsx)(a.h3,{id:"catalog-service",children:"Catalog Service"}),"\n",(0,n.jsx)(a.p,{children:"The core of OpenHouse's control plane is a RESTful Table Service that provides secure and scalable table provisioning\nand declarative metadata management.\nAt the core of the Catalog Service is the Table model. A Table is a logical representation of a dataset in the data.\nIt includes the following metadata:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Identifiers (Cluster ID, Database ID, Table ID, Table URI, Table UUID)"}),"\n",(0,n.jsx)(a.li,{children:"Schema (Column Name, Column Type, Column Comment)"}),"\n",(0,n.jsx)(a.li,{children:"Time Partitioning and String Clustering (Column Name)"}),"\n",(0,n.jsx)(a.li,{children:"Table Policies (Retention, Sharing, Tags)"}),"\n",(0,n.jsx)(a.li,{children:"Table Type (Primary, Replica)"}),"\n",(0,n.jsx)(a.li,{children:"Versioning (Table Version)"}),"\n",(0,n.jsx)(a.li,{children:"Other Metadata (Creator, Created Time, Last Modified Time)"}),"\n"]}),"\n",(0,n.jsx)(a.h4,{id:"multi-table-format-support",children:"Multi Table Format Support"}),"\n",(0,n.jsx)(a.p,{children:"Catalog Service APIs are designed to support multiple table formats. Currently, it supports Iceberg and POC for Delta is\nWIP. To be able to extend to multiple table formats, following considerations need to be made:"}),"\n",(0,n.jsxs)(a.ol,{children:["\n",(0,n.jsx)(a.li,{children:"Base Table API, Metadata and Policies are designed to be format agnostic. For example, Retention, Sharing are generic\nuse-cases across all table formats."}),"\n",(0,n.jsxs)(a.li,{children:["Schema representation is serialized string. This allows to use the ",(0,n.jsx)(a.a,{href:"https://github.com/apache/iceberg/blob/main/api/src/main/java/org/apache/iceberg/Schema.java",children:"Iceberg Schema"}),"\nfor Iceberg tables and for Delta tables use the ",(0,n.jsx)(a.a,{href:"https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/types/StructType.html",children:"Spark StructType"})]}),"\n",(0,n.jsx)(a.li,{children:"In case there is a need to represent format specific metadata in the API, it is done through the API extensions as\nshown below:"}),"\n"]}),"\n",(0,n.jsx)(a.pre,{children:(0,n.jsx)(a.code,{children:"/dabatase/{databaseId}/table/{tableId}/ --\x3e Base Table API\n/database/{databaseId}/table/{tableId}/iceberg/v2/snapshots --\x3e Iceberg specific API extensions\n/databases/{databaseId}/tables/{tableId}/delta/v1/actions --\x3e Delta specific API extensions\n"})}),"\n",(0,n.jsx)(a.h4,{id:"committing-table-metadata",children:"Committing Table Metadata"}),"\n",(0,n.jsx)(a.p,{children:"When a table is created or updated, catalog service writes the table metadata to the table format specific metadata\nfile on storage. In case of Iceberg tables, the service writes all the Openhouse specific table metadata to Iceberg\ntable's root metadata json file. The service then writes the location of this root metadata to the House Table Service\nthrough an atomic compare version and swap version and file location."}),"\n",(0,n.jsx)(a.h3,{id:"house-table-service",children:"House Table Service"}),"\n",(0,n.jsxs)(a.p,{children:["House Table Service is a RESTful service designed to provide a key-value API for storing and retrieving\nmetadata for ",(0,n.jsx)(a.a,{href:"#catalog-service",children:"Catalog Service"})," and ",(0,n.jsx)(a.a,{href:"#jobs-service",children:"Jobs Service"}),". The backend storage for House Table\nService is pluggable, and can use a Database supported by ",(0,n.jsx)(a.a,{href:"https://spring.io/projects/spring-data",children:"Spring Data JPA"}),".\nThe default implementation uses H2 in-memory, but it can be extended to use other databases like MySQL, Postgres, etc."]}),"\n",(0,n.jsx)(a.p,{children:"Note, house table service is an internal service, and should only be accessed via Catalog Service or Jobs Service."}),"\n",(0,n.jsx)(a.h3,{id:"data-services",children:"Data Services"}),"\n",(0,n.jsx)(a.p,{children:"Jobs Scheduler and Jobs Service are the two components that are responsible for orchestrating the execution of data\nservices. These components are designed to be modular and can be extended to support multiple data services."}),"\n",(0,n.jsx)(a.h4,{id:"jobs-scheduler",children:"Jobs Scheduler"}),"\n",(0,n.jsx)(a.p,{children:"Jobs Scheduler (at /apps/spark) is responsible for iterating through all the tables, and for each table, it will trigger\nthe corresponding job type by calling the Jobs Service endpoint. It can be run as a cronjob per job type. Currently,\nfollowing job types are supported:"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Retention"}),"\n",(0,n.jsx)(a.li,{children:"Iceberg Snapshot Expiration"}),"\n",(0,n.jsx)(a.li,{children:"Iceberg Orphan File Deletion"}),"\n",(0,n.jsx)(a.li,{children:"Iceberg Staged File Deletion"}),"\n"]}),"\n",(0,n.jsxs)(a.p,{children:["Jobs Scheduler integrates with the ",(0,n.jsx)(a.a,{href:"#catalog-service",children:"Catalog Service"})," to get the list of tables and corresponding\nmetadata. For example, if a table has a retention policy of 30 days, Jobs Scheduler will trigger the Job Service with\nthe table name and retention period."]}),"\n",(0,n.jsx)(a.h4,{id:"jobs-service",children:"Jobs Service"}),"\n",(0,n.jsxs)(a.p,{children:["All the job types discussed in ",(0,n.jsx)(a.a,{href:"#jobs-scheduler",children:"Jobs Scheduler"})," are implemented as Spark applications, and it is the\nJobs Service (at /services/jobs) that is responsible for submitting these jobs to the Spark cluster. Jobs Services is a\nRESTful service that provides an endpoint for the scheduler to trigger a job. Jobs Service is modularized in a way that\nit can be extended to support various spark job submission APIs. One such implementation based on Apache Livy can be\nfound at ",(0,n.jsx)(a.em,{children:"services/jobs"}),", and it is used in docker compose setup."]}),"\n",(0,n.jsxs)(a.p,{children:["Jobs Service also maintains all the job metadata in the ",(0,n.jsx)(a.a,{href:"#house-table-service",children:"House Table Service"}),", including the\nstatus of the job, the time it was triggered, table name, table metadata, error logs, and job type. This metadata is\nused for tracking job completions, observability and monitoring purposes."]}),"\n",(0,n.jsx)(a.h2,{id:"engine-integration",children:"Engine Integration"}),"\n",(0,n.jsxs)(a.p,{children:["OpenHouse is designed to integrate with various engines like Spark, Trino, and Flink at their Catalog layer. For Iceberg\ntables, OpenHouseCatalog (at /integrations/spark) implementation is provided that extends Iceberg's ",(0,n.jsx)(a.a,{href:"https://github.com/apache/iceberg/blob/main/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java",children:"BaseMetastoreCatalog"}),"\nGiven OpenHouse supports Table Sharing and Policy Management through Spark SQL syntax, SQL extensions are created, and\nimplemented in the OpenHouseCatalog."]}),"\n",(0,n.jsx)(a.p,{children:"The engine side integration with OpenHouseCatalog leverages a lower level REST client to interact with the Catalog\nService. This REST client is autogenerated as part of the build process from the OpenAPI documentation as described in\nthe Client Codegen (docs/development/client-code-generation.md) guide."}),"\n",(0,n.jsx)(a.p,{children:"Trino and Flink engines follow a similar pattern of integration with OpenHouse and are work in progress."}),"\n",(0,n.jsx)(a.h2,{id:"deployed-system-architecture",children:"Deployed System Architecture"}),"\n",(0,n.jsx)(a.p,{children:(0,n.jsx)(a.img,{alt:"Example Deployed System Architecture",src:t(9009).Z+"",width:"1024",height:"584"})}),"\n",(0,n.jsx)(a.p,{children:"The figure above shows system components of OpenHouse deployed at LinkedIn. Each component is numbered and its purpose\nis as follows:"}),"\n",(0,n.jsx)(a.p,{children:"[1] Catalog (/Table) service: This is a RESTful web service that exposes tables REST resources. This service is deployed\non a Kubernetes cluster with a fronting Envoy Network Proxy."}),"\n",(0,n.jsx)(a.p,{children:"[2] REST clients: A variety of applications use REST clients to call into table service (#1). Clients include but are\nnot limited to compliance apps, replication apps, data discovery apps like Datahub and IaC, Terraform providers, and\ndata quality checkers. Some of the apps that work on all the tables in OpenHouse are assigned higher privileges."}),"\n",(0,n.jsx)(a.p,{children:"[3] Metastore Catalog: Spark,Trino, andFlink engines are a special flavor of REST clients. An OpenHouse specific\nmetastore catalog implementation allows engines to integrate with OpenHouse tables."}),"\n",(0,n.jsx)(a.p,{children:"[4] House Database (/Table) service: This is an internal service to store table service and data service metadata. This\nservice exposes a key-value interface that is designed to use a NoSQL DB for scale and cost optimization. However the\ndeployed system is currently backed by a MySQL instance, for ease of development and deployment."}),"\n",(0,n.jsx)(a.p,{children:"[5] Managed namespace: This is a managed HDFS namespace where tables are persisted in Iceberg table format. Table\nservice is responsible for setting up the table directory structure with appropriate FileSystem permissioning.\nOpenHouse has a novel HDFS permissioning scheme that makes it possible for any ETL flow to publish directly to Iceberg tables and securely into a managed HDFS namespace."}),"\n",(0,n.jsx)(a.p,{children:"[6] Data services: This is a set of data services that reconciles the user / system declared configuration with the\nsystem observed configuration. This includes use cases such as retention, restatement, and Iceberg-specific maintenance.\nEach maintenance activity is scheduled as a Spark job per table. A Kubernetes cronjob is run periodically on a schedule to trigger a maintenance activity. All the bookkeeping of jobs is done in House Database Service using a jobs metadata table for ease of debugging and monitoring."}),"\n",(0,n.jsx)(a.h2,{id:"pluggable-architecture",children:"Pluggable Architecture"}),"\n",(0,n.jsx)(a.p,{children:"Components of OpenHouse are designed to be pluggable, so that OpenHouse can be deployed in diverse private and public\ncloud environments. This pluggability is available for the following components (Note: Integrations with a (*) are\navailable, and for other integrations, APIs are defined but not implemented yet.)"}),"\n",(0,n.jsxs)(a.ul,{children:["\n",(0,n.jsx)(a.li,{children:"Storage backend for example, HDFS(*), Blob Stores, etc."}),"\n",(0,n.jsxs)(a.li,{children:["File Formats for example, Orc(",(0,n.jsx)(a.em,{children:"), Parquet("}),")."]}),"\n",(0,n.jsxs)(a.li,{children:["Database for ",(0,n.jsx)(a.a,{href:"#house-table-service",children:"House Table Service"})," for example, MySQL(*), Postgres, etc."]}),"\n",(0,n.jsxs)(a.li,{children:["Job Submission API for ",(0,n.jsx)(a.a,{href:"#jobs-service",children:"Jobs Service"})," for example, Apache Livy(*), Spark REST etc."]}),"\n",(0,n.jsx)(a.li,{children:"Custom Authentication and Authorization handlers for all the services specific to an environment."}),"\n"]}),"\n",(0,n.jsx)(a.p,{children:"OpenHouse Cluster Configuration Spec (at /cluster/configs) is the place where all the configurations are defined for a\nparticular cluster setup."})]})}function h(e={}){const{wrapper:a}={...(0,i.a)(),...e.components};return a?(0,n.jsx)(a,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},1369:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/openhouse-controlplane-91ace254c8643ca21b4dae37692b4299.jpeg"},9009:(e,a,t)=>{t.d(a,{Z:()=>n});const n=t.p+"assets/images/openhouse-deployed-architecture-94d43e6dfe0c0035ada5755c63a53310.jpeg"},1151:(e,a,t)=>{t.d(a,{Z:()=>o,a:()=>r});var n=t(7294);const i={},s=n.createContext(i);function r(e){const a=n.useContext(s);return n.useMemo((function(){return"function"==typeof e?e(a):{...a,...e}}),[a,e])}function o(e){let a;return a=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),n.createElement(s.Provider,{value:a},e.children)}}}]);